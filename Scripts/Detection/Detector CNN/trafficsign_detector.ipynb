{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import inspect as inspect\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gtsrb\\Desktop\\detector\n",
      "C:\\Users\\gtsrb\\Desktop\\detector\\dataset_Train.csv\n",
      "C:\\Users\\gtsrb\\Desktop\\detector\\dataset_Test.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the paths to the train and test directories\n",
    "root = os.getcwd() \n",
    "print(root)\n",
    "train_data = os.path.join(root, r'dataset_Train.csv')\n",
    "print(train_data)\n",
    "test_data = os.path.join(root, r'dataset_Test.csv')\n",
    "print(test_data)\n",
    "#label_file = os.path.join(root, r'archive\\TrainIJCNN2013\\TrainIJCNN2013\\gt.txt')\n",
    "#print(label_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images and labels into arrays\n",
    "X_train = []\n",
    "y_train = []\n",
    "train_data = pd.read_csv(train_data)\n",
    "for i, row in train_data.iterrows():\n",
    "    image = Image.open(row['Path'])\n",
    "    #image = image.resize((32, 32))  # Resize the image to 32x32 pixels\n",
    "    X_train.append(np.array(image))\n",
    "    y_train.append(row['Bonding Boxs'])\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "#print(str(np.size(X_train)) + ' ' + str(X_train[0])) # size = 2780928000\n",
    "#print(str(np.size(y_train)) + ' ' + str(y_train[0])) # size = 852\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "test_data = pd.read_csv(test_data)\n",
    "for i, row in test_data.iterrows():\n",
    "    image = Image.open(row['Path'])\n",
    "    # image = image.resize((32, 32))  # Resize the image to 32x32 pixels\n",
    "    X_test.append(np.array(image))\n",
    "    y_test.append(row['Bonding Boxs'])\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#print(str(np.size(X_test)) + ' ' + str(X_test[0])) # size = 1178304000\n",
    "#print(str(np.size(y_test)) + ' ' + str(y_test[0])) # size = 361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train = X_train.astype('float32') / 255  # Normalize pixel values to be between 0 and 1\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "y_train =  np.array([list(map(np.float32, s.split(';'))) for s in y_train])\n",
    "y_test =  np.array([list(map(np.float32, s.split(';'))) for s in y_test])\n",
    "\n",
    "#print(str(np.size(X_train)) + ' ' + str(X_train[0]))\n",
    "#print(str(np.size(y_train)) + ' ' + str(y_train[:][:]))\n",
    "#print(str(np.size(X_test)) + ' ' + str(X_test[0]))\n",
    "#print(str(np.size(y_test)) + ' ' + str(y_test[:][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train.transpose((0, 3, 1, 2)))  # Transpose to match PyTorch's tensor format (NCHW)\n",
    "y_train = torch.tensor(y_train).unsqueeze(-1)\n",
    "X_test = torch.tensor(X_test.transpose((0, 3, 1, 2)))  # Transpose to match PyTorch's tensor format (NCHW)\n",
    "y_test = torch.tensor(y_test).unsqueeze(-1)\n",
    "\n",
    "#print(str(X_train.size()) + ' ' + str(X_train[0][0]))\n",
    "#print(str(np.size(y_train)) + ' ' + str(y_train[:][:]))\n",
    "#print(str(X_test.size()) + ' ' + str(X_test[0][0]))\n",
    "#print(str(np.size(y_test)) + ' ' + str(y_test[:][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConvLayer(nn.Module):\n",
    "    def __init__(self, min_kernel_size):\n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.min_kernel_size = min_kernel_size\n",
    "        self.conv = nn.Conv2d(3, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, bbox_size):\n",
    "        print(bbox_size)\n",
    "        min_bbox_size = bbox_size.min()\n",
    "        kernel_size = max(self.min_kernel_size, min_bbox_size.item())\n",
    "        print(kernel_size)\n",
    "        padding = (kernel_size -1) // 2\n",
    "        conv = nn.functional.conv2d(x, self.conv.weight, stride=1, padding=padding)\n",
    "        return conv(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model architecture\n",
    "# class TrafficSignDetector(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(TrafficSignDetector, self).__init__()\n",
    "#         self.costum_conv = CustomConvLayer(min_kernel_size = 9)\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         #self.fc1 = nn.Linear(64 * 8 * 8, 64)\n",
    "#         self.fc2_presence = nn.Linear(64, 1)  # Output for presence\n",
    "#         self.fc2_bbox = nn.Linear(64, 4)  # Output for bounding box coordinates\n",
    "\n",
    "#     def forward(self, x, bounding_boxes):\n",
    "#         x = self.costum_conv(x, calculate_bounding_box_size(bounding_boxes))\n",
    "#         x = self.pool(torch.relu(self.conv1(x)))\n",
    "#         x = self.pool(torch.relu(self.conv2(x)))\n",
    "#         print('before: ' + str(x.size()))\n",
    "#         #x = x.reshape(-1, 64 * 8 * 8)\n",
    "#         print('after: ' + str(x.size()))\n",
    "#         #x = torch.relu(self.fc1(x))\n",
    "#         presence = torch.sigmoid(self.fc2_presence(x))\n",
    "#         bbox = self.fc2_bbox(x)\n",
    "#         return presence, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignDetector(nn.Module):\n",
    "    def __init__(self, min_kernel_size):\n",
    "        super(TrafficSignDetector, self).__init__()\n",
    "        #self.custom_conv = CustomConvLayer(min_kernel_size)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(8, 2)\n",
    "        self.fc1 = nn.Linear(256 * 94 * 164, 512)\n",
    "        self.fc2 = nn.Linear(512, 5)  # Output: (confidence, x, y, width, height)\n",
    "\n",
    "    def forward(self, x, bbox):\n",
    "        #x = self.custom_conv(x, calculate_bounding_box_size(bbox_size))  # Apply custom convolutional layer\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        bbox = bbox.unsqueeze(-1)\n",
    "        bbox = bbox.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x = torch.cat((x, bbox), dim=1)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.flatten(start_dim=1) # flatten, nur batch size ganz vorne beibehalten\n",
    "        #x = x.view(-1, 256 * 8 * 8)  # Flatten before fully connected layers\n",
    "        #x = x.reshape(-1, 256 * 8 * 8)  # Flatten before fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bounding_box_size(bounding_boxes):\n",
    "    box_sizes = []\n",
    "#    for bbox in bounding_boxes:\n",
    "#         x_min = bbox[0]\n",
    "#         y_min = bbox[1]\n",
    "#         x_max = bbox[2]\n",
    "#         y_max = bbox[3] \n",
    "#         width = x_max - x_min\n",
    "#         height = y_max - y_min\n",
    "#         box\n",
    "    box_sizes = bounding_boxes[:, 2:] - bounding_boxes[:, :2]\n",
    "    box_widths = box_sizes[:, 0]\n",
    "    box_heights = box_sizes[:, 1]\n",
    "    print(\"Widths:\", box_widths)\n",
    "    print(\"Heights:\", box_heights)\n",
    "    \n",
    "    all_sizes = torch.cat((box_sizes[:, 0].unsqueeze(1), box_sizes[:, 1].unsqueeze(1)), dim=1)\n",
    "    print(all_sizes)\n",
    "        \n",
    "    return all_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "#model = TrafficSignDetector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 68, 397, 677] to have 64 channels, but got 68 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#shape(batch_y) = torch.Size([64, 4])\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#shape(batch_x) = torch.Size([64, 3, 800, 1360])\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_x, batch_y)  \n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)  \u001b[38;5;66;03m# Use the first element of bbox for training\u001b[39;00m\n\u001b[0;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m, in \u001b[0;36mTrafficSignDetector.forward\u001b[1;34m(self, x, bbox)\u001b[0m\n\u001b[0;32m     16\u001b[0m bbox \u001b[38;5;241m=\u001b[39m bbox\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, bbox), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# flatten, nur batch size ganz vorne beibehalten\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 68, 397, 677] to have 64 channels, but got 68 channels instead"
     ]
    }
   ],
   "source": [
    "# Initialize your model\n",
    "min_kernel_size = 3  # You can adjust this as needed\n",
    "model = TrafficSignDetector(min_kernel_size)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Use Mean Squared Error loss since it's a regression task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  \n",
    "batch_size = 64  \n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_x = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #shape(batch_y) = torch.Size([64, 4])\n",
    "        #shape(batch_x) = torch.Size([64, 3, 800, 1360])\n",
    "        outputs = model(batch_x, batch_y)  \n",
    "        loss = criterion(outputs, batch_y)  # Use the first element of bbox for training\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i//batch_size+1}/{len(X_train)//batch_size}], Loss: {loss.item():.4f}, Outputs: \\n {outputs.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "# criterion_presence = nn.BCELoss()\n",
    "# criterion_bbox = nn.SmoothL1Loss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# num_batches = len(X_train) // 32\n",
    "# num_epochs = 5\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0\n",
    "#     for batch in range(num_batches):\n",
    "#         batch_X = X_train[batch * 32 : (batch + 1) * 32]\n",
    "#         batch_y = y_train[batch * 32 : (batch + 1) * 32]\n",
    "        \n",
    "#         print('batch_X: '+ str(batch_X.size()) + ' ' + str(batch_X.shape))\n",
    "#         print('batch_y: '+ str(batch_y.size()) + ' ' + str(batch_y.shape))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         #outputs = model(batch_X, batch_y)\n",
    "\n",
    "#         # Forward pass\n",
    "#         presence_pred, bbox_pred = model(batch_X, batch_y)\n",
    "        \n",
    "#         # Compute the loss for presence prediction\n",
    "#         presence_target = torch.tensor([[1.0] if bbox.any() else [0.0] for bbox in batch_y])\n",
    "#         print('presence_target: '+ str(presence_target.size()) + ' ' + str(presence_target.shape))\n",
    "#         presence_loss = criterion_presence(presence_pred, presence_target)\n",
    "\n",
    "#           # Compute the loss for bounding box regression\n",
    "#         bbox_loss = criterion_bbox(bbox_pred, batch_y)\n",
    "\n",
    "#         # Combine the losses\n",
    "#         loss = presence_loss + bbox_loss\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "        \n",
    "#         # Compute the loss\n",
    "#         #batch_y = batch_y.long()\n",
    "#         #loss = criterion(outputs, batch_y)\n",
    "#         #total_loss += loss.item()\n",
    "\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Print average loss for the epoch\n",
    "#     average_loss = total_loss / num_batches\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,r'.\\cache02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(r'.\\cache.h5')\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "model.eval()\n",
    "batch_size = 1\n",
    "with torch.no_grad():\n",
    "    for batch in range(0, len(X_test)):\n",
    "        batch_x = X_test[batch:batch + batch_size]\n",
    "        batch_y = y_test[batch:batch + batch_size]\n",
    "        \n",
    "        print(str(batch) + ': ' + str(batch_y))\n",
    "        \n",
    "        outputs = model(batch_x, batch_y)\n",
    "        x_min = str(outputs[:, 1].item() * (100 - 0) + 0)\n",
    "        y_min = str(outputs[:, 2].item() * (100 - 0) + 0)\n",
    "        x_max = str(outputs[:, 1].item() + outputs[:, 3].item())\n",
    "        y_max = str(outputs[:, 2].item() + outputs[:, 4].item())\n",
    "        print(str(batch) + ': ' + str(outputs[:,0]) + '; ' + x_min + ', ' + y_min + ', '  + x_max + ', ' + y_max )\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vid(path):\n",
    "    video = cv2.VideoCapture(path)\n",
    "    \n",
    "    if not video.isOpened():\n",
    "        print(\"Fehler beim laden des Videos: \" + path)\n",
    "        return []\n",
    "    \n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save frames for processing\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "        #cv2.imshow('Video' , frame)\n",
    "\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            #break\n",
    "\n",
    "    video.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "    \n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path_list = []\n",
    "frames_lists = []\n",
    "\n",
    "for name, _, datas in os.walk(r'C:\\Users\\gtsrb\\Desktop\\detector\\videos'):\n",
    "    for data in datas:\n",
    "        video_path = os.path.join(name, data)\n",
    "\n",
    "        if any(video_path.endswith(ext) for ext in ['.mp4', '.avi','.mkv']):\n",
    "            video_path_list.append(video_path)\n",
    "\n",
    "for video_path in video_path_list:\n",
    "   frames_lists.append(load_vid(video_path))\n",
    "   #print(video_list[len(video_list)-1])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(video_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "frames_lists = np.array(frames_lists)\n",
    "frames_lists = frames_lists.astype('float32')/255\n",
    "frames_lists = torch.tensor(frames_lists.transpose((0, 3, 1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = torch.load(r'.\\cache.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(frames_lists) // 32\n",
    "threshold=0.5 \n",
    "\n",
    "frame_count = 0\n",
    "for batch in range(0, len(frames_lists), 64):\n",
    "    batch_frames = frames_lists[batch: batch + 64]\n",
    "    \n",
    "    optimizer.zero_grad\n",
    "\n",
    "    outputs = model(batch_frames)\n",
    "\n",
    "    predictions = outputs.squeeze()\n",
    "    traffic_sign_present = predictions >= threshold\n",
    "    \n",
    "    if traffic_sign_present.any():\n",
    "         current_frame = batch_frames[0].numpy()\n",
    "         frame_path = r\"./cache/frame_\" + str(frame_count) + str(output[:, 4]) + \".jpg\"\n",
    "         cv2.imwrite(frame_path, current_frame)\n",
    "         print(\"Traffic sign detected in the current frame. Saved as frame_\" + str(frame_count) + str(output[:, 4]) + \".jpg\")\n",
    "         frame_count += 1\n",
    "\n",
    "    loss = criterion(outputs)\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "357.844px",
    "left": "1522px",
    "right": "20px",
    "top": "120px",
    "width": "322px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
